import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
import joblib
import numpy as np
import pandas as pd
import json
import os

class ImprovedFraudGNN(torch.nn.Module):
    def __init__(self, num_features):
        super().__init__()
        self.conv1 = GCNConv(num_features, 64)
        self.conv2 = GCNConv(64, 32)
        self.conv3 = GCNConv(32, 16)
        self.classifier = torch.nn.Linear(32, 2)
        self.dropout = torch.nn.Dropout(0.5)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.dropout(x)
        x = F.relu(self.conv2(x, edge_index))
        return self.classifier(x)

def load_artifacts(artifacts_dir: str):
    """
    Táº£i cÃ¡c thÃ nh pháº§n cá»§a mÃ´ hÃ¬nh tá»« má»™t thÆ° má»¥c Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh.
    Sá»­a lá»—i: Cháº¥p nháº­n má»™t Ä‘á»‘i sá»‘ lÃ  Ä‘Æ°á»ng dáº«n thÆ° má»¥c.
    """
    pipeline_path = os.path.join(artifacts_dir, 'preprocessing_pipeline.pkl')
    metadata_path = os.path.join(artifacts_dir, 'metadata.json')
    weights_path = os.path.join(artifacts_dir, 'fraud_gnn_weights.pth')

    if not all(os.path.exists(p) for p in [pipeline_path, metadata_path, weights_path]):
        raise FileNotFoundError(f"Má»™t hoáº·c nhiá»u file mÃ´ hÃ¬nh khÃ´ng Ä‘Æ°á»£c tÃ¬m tháº¥y trong thÆ° má»¥c: {artifacts_dir}. Vui lÃ²ng kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n.")

    pipeline = joblib.load(pipeline_path)
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)

    final_features_list = metadata['features']['final_features_list']
    num_features = len(final_features_list)

    model = ImprovedFraudGNN(num_features)
    model.load_state_dict(torch.load(weights_path, map_location='cpu'))
    model.eval()

    # Tráº£ vá» cáº£ danh sÃ¡ch cÃ¡c Ä‘áº·c trÆ°ng mong muá»‘n Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng
    return model, pipeline, final_features_list

def predict_address(model, pipeline, features_dict: dict, expected_columns: list):
    """
    Dá»± Ä‘oÃ¡n má»™t Ä‘á»‹a chá»‰.
    Tá»‘i Æ°u: Nháº­n 'expected_columns' trá»±c tiáº¿p thay vÃ¬ Ä‘á»c láº¡i file metadata má»—i láº§n gá»i.
    """
    df = pd.DataFrame([features_dict])

    # Sáº¯p xáº¿p láº¡i vÃ  Ä‘iá»n cÃ¡c cá»™t bá»‹ thiáº¿u Ä‘á»ƒ khá»›p vá»›i Ä‘áº§u vÃ o cá»§a mÃ´ hÃ¬nh
    missing_cols = set(expected_columns) - set(df.columns)
    for col in missing_cols:
        df[col] = 0.0
    df = df[expected_columns]

    x_proc = pipeline.transform(df)
    x_tensor = torch.tensor(x_proc, dtype=torch.float32)
    edge_index = torch.tensor([], dtype=torch.long).reshape(2, 0)

    with torch.no_grad():
        logits = model(x_tensor, edge_index)
        probs = torch.softmax(logits, dim=1)[0]
        pred_index = int(torch.argmax(probs))
        confidence_percent = float(probs[pred_index]) * 100

    status = "fraud" if pred_index == 1 else "non-fraud"
    return status, (confidence_percent / 100), confidence_percent

def explain_address(model, pipeline, features_dict, feat_names, topk=None):  # ThÃªm topk optional Ä‘á»ƒ khá»›p vá»›i app.py, nhÆ°ng bá» qua nÃ³
    # TÆ°Æ¡ng tá»±, thÃªm reorder cho hÃ m nÃ y Ä‘á»ƒ trÃ¡nh lá»—i náº¿u gá»i
    with open('../Model/metadata.json', 'r') as f:
        metadata = json.load(f)
    expected_columns = metadata['features']['final_features_list']  # Giá»¯ key nhÆ° attachment

    df = pd.DataFrame([features_dict])

    # Debug: In Ä‘á»ƒ kiá»ƒm tra trÆ°á»›c reorder
    print("ğŸ” Explain features columns before reorder:", df.columns.tolist())

    # Reorder vÃ  fill default
    missing_cols = set(expected_columns) - set(df.columns)
    for col in missing_cols:
        df[col] = 0.0
    df = df[expected_columns]

    # Debug: In sau reorder
    print("âœ… Explain features columns after reorder:", df.columns.tolist())

    x_proc = pipeline.transform(df)
    x_tensor = torch.tensor(x_proc, dtype=torch.float32)
    importance = model.conv1.lin.weight.abs().sum(dim=0).detach().numpy()
    feat_names_aligned = feat_names if len(feat_names) == len(importance) else [f"f{i}" for i in range(len(importance))]

    # Sáº¯p xáº¿p táº¥t cáº£ features theo importance giáº£m dáº§n (bá» qua topk)
    idx = importance.argsort()[::-1]  # Tá»« cao Ä‘áº¿n tháº¥p
    all_features = np.array(feat_names_aligned)[idx]
    all_importance = importance[idx].round(3)

    # Convert numpy.float32 sang float Ä‘á»ƒ serialize JSON
    feature_importance = {feat: float(value) for feat, value in zip(all_features, all_importance)}

    return {
        "explanation": "Giáº£i thÃ­ch importance cá»§a táº¥t cáº£ cÃ¡c Ä‘áº·c trÆ°ng (sáº¯p xáº¿p tá»« áº£nh hÆ°á»Ÿng cao nháº¥t Ä‘áº¿n tháº¥p nháº¥t)",
        "feature_importance": feature_importance
    }
