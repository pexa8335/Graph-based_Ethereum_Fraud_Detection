# -*- coding: utf-8 -*-
"""Ethereum_Retrain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EbTA77nwM2HfP65vQJEhh_KuVOHZzIUn
"""

#!pip install scikit-learn==1.5.1 --force-reinstall

import sklearn
print("‚úÖ ƒêang d√πng sklearn version:", sklearn.__version__)

path = '/content/transaction_dataset.csv'

from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.pipeline import Pipeline
from ml_transformers import ColumnDropper, ControlCharacterCleaner, IntelligentImputer
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.preprocessing import StandardScaler

def train_val_test_split(df, train_size=0.7, val_size=0.2, test_size=0.1, random_state=42):
    """
    Chia DataFrame th√†nh train-val-test theo t·ªâ l·ªá ch·ªâ ƒë·ªãnh
    """
    # Step 1: Chia train+val vs test
    train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)

    # Step 2: Chia train vs val
    val_ratio = val_size / (train_size + val_size)
    train_df, val_df = train_test_split(train_val_df, test_size=val_ratio, random_state=random_state)

    return train_df, val_df, test_df

# Th·ª±c hi·ªán chia d·ªØ li·ªáu
df = pd.read_csv(path, index_col = 0)
df.columns = df.columns.str.strip()
train_df, val_df, test_df = train_val_test_split(df, random_state=42)

print(f"üìä K√≠ch th∆∞·ªõc datasets:")
print(f"Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)")
print(f"Val: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)")
print(f"Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)")

# üöÄ B∆Ø·ªöC 3: Chu·∫©n b·ªã features v√† targets
X_train = train_df.drop('FLAG', axis=1)
y_train = train_df['FLAG']

X_val = val_df.drop('FLAG', axis=1)
y_val = val_df['FLAG']

X_test = test_df.drop('FLAG', axis=1)
y_test = test_df['FLAG']

features_to_drop_experiment = [
    'Index',
    'Address',
]

column_dropper = ColumnDropper(columns_to_drop=features_to_drop_experiment)

# üëâ T·∫°o X_temp ƒë·ªÉ l·∫•y t√™n c·ªôt sau khi drop
X_temp = column_dropper.fit_transform(X_train)
final_feature_names = X_temp.columns.tolist()  # üëà t√™n c√°c feature cu·ªëi c√πng

preprocessing_pipeline = Pipeline(steps=[
    ('column_dropper', column_dropper),
    ('control_char_cleaner', ControlCharacterCleaner()),
    ('intelligent_imputer', IntelligentImputer()),
    ('scaler', StandardScaler())
  ])
# Fit-transform
X_train_processed = preprocessing_pipeline.fit_transform(X_train)
X_val_processed = preprocessing_pipeline.transform(X_val)
X_test_processed = preprocessing_pipeline.transform(X_test)


print("üîß Preprocessing data...")
X_train_processed = pd.DataFrame(X_train_processed, columns=final_feature_names)
X_val_processed = pd.DataFrame(X_val_processed, columns=final_feature_names)
X_test_processed = pd.DataFrame(X_test_processed, columns=final_feature_names)

print(f"‚úÖ Features shape: {X_train_processed.shape}")

import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import classification_report, f1_score
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# --- 1. Graph builder ---
def build_fraud_aware_graph(X, y):
    X_array = X.values if hasattr(X, 'values') else X
    y_array = y.values if hasattr(y, 'values') else y

    knn = NearestNeighbors(n_neighbors=3, metric='cosine')
    knn.fit(X_array)
    distances, indices = knn.kneighbors(X_array)

    edges = []
    for i in range(len(X_array)):
        for j in range(1, 3):
            neighbor = indices[i][j]
            similarity = 1 - distances[i][j]
            if similarity > 0.6:
                edges.append([i, neighbor])
                edges.append([neighbor, i])

    x = torch.tensor(X_array, dtype=torch.float32)
    y_tensor = torch.tensor(y_array, dtype=torch.long)
    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long)

    return Data(x=x, edge_index=edge_index, y=y_tensor)

# --- 2. Model ---
class ImprovedFraudGNN(torch.nn.Module):
    def __init__(self, num_features):
        super(ImprovedFraudGNN, self).__init__()
        self.conv1 = GCNConv(num_features, 64)
        self.conv2 = GCNConv(64, 32)
        self.conv3 = GCNConv(32, 16)
        self.classifier = torch.nn.Linear(32, 2)
        self.dropout = torch.nn.Dropout(0.5)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.dropout(x)
        x = F.relu(self.conv2(x, edge_index))
        return self.classifier(x)

# --- 3. Build graphs ---
print("üîß Building graphs...")
train_data = build_fraud_aware_graph(X_train_processed, y_train)
val_data = build_fraud_aware_graph(X_val_processed, y_val)
test_data = build_fraud_aware_graph(X_test_processed, y_test)
print(f"Train graph: {train_data.x.shape[0]} nodes, {train_data.edge_index.shape[1]} edges")

# --- 4. Class weights ---
y_train_array = train_data.y.cpu().numpy()
class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train_array)
class_weights = torch.tensor(class_weights, dtype=torch.float32)

# --- 5. Setup ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = ImprovedFraudGNN(num_features=train_data.x.shape[1]).to(device)

train_data = train_data.to(device)
val_data = val_data.to(device)
test_data = test_data.to(device)
class_weights = class_weights.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
criterion = torch.nn.CrossEntropyLoss(weight=class_weights)

# --- 6. Training with best model tracking ---
print("üöÄ Training GNN...")

best_model_state = None
best_val_f1 = 0.0

for epoch in range(300):
    model.train()
    optimizer.zero_grad()
    out = model(train_data.x, train_data.edge_index)
    loss = criterion(out, train_data.y)
    loss.backward()
    optimizer.step()

    # Evaluate every 30 epochs (or you can do every epoch)
    model.eval()
    with torch.no_grad():
        val_out = model(val_data.x, val_data.edge_index)
        val_pred = val_out.argmax(dim=-1)
        val_f1 = f1_score(val_data.y.cpu(), val_pred.cpu(), pos_label=1, zero_division=0)

        # ‚úÖ Save best model
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_model_state = model.state_dict()

    if epoch % 30 == 0:
        print(f"Epoch {epoch}: Loss={loss:.4f}, Val F1={val_f1:.4f}")

# --- 7. Load best model before final evaluation ---
model.load_state_dict(best_model_state)
model.eval()
with torch.no_grad():
    test_out = model(test_data.x, test_data.edge_index)
    test_pred = test_out.argmax(dim=-1)

print("\nüéØ GNN RESULTS")
print("=" * 30)
print(classification_report(test_data.y.cpu(), test_pred.cpu(), target_names=['Normal', 'Fraud']))

import os
import joblib
import torch
import json
from datetime import datetime

# --- üîß ƒê∆∞·ªùng d·∫´n l∆∞u artifacts ---
ARTIFACTS_DIR = 'deployment_artifacts/'
os.makedirs(ARTIFACTS_DIR, exist_ok=True)

PREPROCESSING_PIPELINE_PATH = os.path.join(ARTIFACTS_DIR, 'preprocessing_pipeline.pkl')
MODEL_WEIGHTS_PATH = os.path.join(ARTIFACTS_DIR, 'fraud_gnn_weights.pth')
METADATA_PATH = os.path.join(ARTIFACTS_DIR, 'metadata.json')

# --- üì¶ 1. L∆∞u pipeline ti·ªÅn x·ª≠ l√Ω ---
try:
    joblib.dump(preprocessing_pipeline, PREPROCESSING_PIPELINE_PATH)
    print(f"‚úÖ Saved preprocessing pipeline ‚Üí {PREPROCESSING_PIPELINE_PATH}")
except NameError:
    print("‚ùå ERROR: Variable `preprocessing_pipeline` not found.")

# --- üíæ 2. L∆∞u tr·ªçng s·ªë c·ªßa model t·ªët nh·∫•t ---
try:
    torch.save(best_model_state, MODEL_WEIGHTS_PATH)
    print(f"‚úÖ Saved best GNN weights ‚Üí {MODEL_WEIGHTS_PATH}")
except NameError:
    print("‚ùå ERROR: Variable `best_model_state` not found. Did you track it during training?")

# --- üìë 3. L∆∞u metadata ƒë·ªÉ gi√∫p load l·∫°i m√¥ h√¨nh ---
# An to√†n l·∫•y t√™n c√°c features
try:
    final_features_list = list(X_train_processed.columns)
except Exception as e:
    print(f"‚ö†Ô∏è WARNING: Failed to get final features. {e}")
    final_features_list = []

# N·∫øu b·∫°n c√≥ danh s√°ch feature ƒë√£ b·ªã lo·∫°i (t√πy ch·ªçn)
try:
    features_to_drop_experiment
except NameError:
    features_to_drop_experiment = []

# N·∫øu b·∫°n ƒë√£ l∆∞u best_val_f1
try:
    best_val_f1
except NameError:
    best_val_f1 = None

# N·∫øu b·∫°n ƒë√£ c√≥ test_pred
try:
    test_report_dict = classification_report(
        test_data.y.cpu(), test_pred.cpu(), target_names=['Normal', 'Fraud'], output_dict=True)
except Exception:
    test_report_dict = {}

metadata = {
    'description': 'Trained GNN model to detect Ethereum fraud transactions.',
    'timestamp_utc': datetime.utcnow().isoformat(),
    'model_class_name': 'ImprovedFraudGNN',
    'paths': {
        'preprocessing_pipeline': PREPROCESSING_PIPELINE_PATH,
        'model_weights': MODEL_WEIGHTS_PATH
    },
    'performance': {
        'best_validation_f1_fraud': best_val_f1,
        'final_test_report': test_report_dict
    },
    'features': {
        'original_features_count': len(getattr(X_train, 'columns', [])),
        'dropped_features': features_to_drop_experiment,
        'final_features_list': final_features_list,
        'final_features_count': len(final_features_list)
    }
}

with open(METADATA_PATH, 'w') as f:
    json.dump(metadata, f, indent=4)
print(f"‚úÖ Saved metadata ‚Üí {METADATA_PATH}")

print("\nüéâ All artifacts saved. Ready for deployment!")

paths = '/content/deployment_artifacts/metadata.json'
with open(paths, 'r') as f:
  print(f.read())

!zip -r my_folder.zip /content/deployment_artifacts
from google.colab import files
files.download("my_folder.zip")